{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VamsiMalla13/Vamsi_INFO5731_Spring2020-/blob/main/In_class_exercise_03_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvUggXBfu086"
      },
      "source": [
        "## The third In-class-exercise (2/22/2022, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNvZ5Zvqu088"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypiz28oMu088"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAyG_MmXu089"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "I am mailnly intrested in sentimental analysis carried out on various reviews to know wethere the reviews are good are bad \n",
        "Here first we need to collect the reviews to which  we need to analysie .\n",
        "later we do sentiment analysis on these products to know who well or bad these are rated . \n",
        "By using the previous reviews we can identify the new issues how the product is working in either in ebay or amazon or on any other site.\n",
        "For these we require the below things to be taken place. \n",
        "\n",
        "1)word 2 vec : \n",
        "This helps to group the bag of words , and single word is represented as around 32 dimension vector or more . By this we can obtain the relation between the 2 different words \n",
        "2)Bag of words : \n",
        "Counts the no of words that occured frequently for the machine learing algorithim to understand the data \n",
        "3)Sentence length : \n",
        "Based on the sentence length and describe wether the tweet or review is good or bad , basically the native tweets or feedbacks are very long in length\n",
        "4)Ngrams : This accomidated the multipl words used and genarate the common sentnces obtained using these words together.\n",
        "5)TF-IDF:\n",
        "This help us to identify the most occurance  word in all the reviews and filter the most common words used for better relaibility.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgwlJnAqu08-"
      },
      "source": [
        "Question 2 (20 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmYt9YXYu08_",
        "outputId": "3db59e11-17b2-411e-9904-02a2f48bc953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "New paragraph : ['the histori asia seen collect histori sever distinct peripher coastal region east asia , south asia , southeast asia middl east link interior mass eurasian stepp .', 'see histori middl east outlin south asian histori detail .', \"the coastal peripheri home world 's earliest known civil religion , three region develop earli civil around fertil river valley .\", 'these valley fertil soil rich could bear mani root crop .', 'the civil mesopotamia , india , china share mani similar like exchang technolog idea mathemat wheel .', 'other notion write like develop individu area .', 'citi , state , empir develop lowland .', 'the stepp region long inhabit mount nomad , central stepp , could reach area asian contin .', 'the northern part contin , cover much siberia also inaccess stepp nomad due dens forest tundra .', 'these area siberia spars popul .', 'the centr peripheri kept separ mountain desert .', 'the caucasu , himalaya , karakum desert , gobi desert form barrier stepp horsemen could cross difficulti .', 'while technolog cultur citi dweller advanc , could littl militarili defend mount hord stepp .', 'howev , lowland enough open grassland support larg horsebound forc .', 'thu nomad conquer state middl east soon forc adapt local societi .', 'the spread islam wave islam golden age timurid renaiss , later influenc age islam gunpowd empir .', \"asia 's histori featur major develop seen part world , well event affect region .\", 'these includ trade silk road , spread cultur , languag , religion , diseas throughout afro-eurasian trade .', 'anoth major advanc innov gunpowd mediev china , later develop gunpowd empir , mainli mughal safavid , led advanc warfar use gun .']\n",
            "New paragraph: ['histori asia seen collect histori sever distinct peripher coastal region east asia , south asia , southeast asia middl east link interior mass eurasian stepp .', 'see histori middl east outlin south asian histori detail .', \"coastal peripheri home world 's earliest known civil religion , three region develop earli civil around fertil river valley .\", 'valley fertil soil rich could bear mani root crop .', 'civil mesopotamia , india , china share mani similar like exchang technolog idea mathemat wheel .', 'notion write like develop individu area .', 'citi , state , empir develop lowland .', 'stepp region long inhabit mount nomad , central stepp , could reach area asian contin .', 'northern part contin , cover much siberia also inaccess stepp nomad due den forest tundra .', 'area siberia spar popul .', 'centr peripheri kept separ mountain desert .', 'caucasu , himalaya , karakum desert , gobi desert form barrier stepp horseman could cross difficulti .', 'technolog cultur citi dweller advanc , could littl militarili defend mount hord stepp .', 'howev , lowland enough open grassland support larg horsebound forc .', 'thu nomad conquer state middl east soon forc adapt local societi .', 'spread islam wave islam golden age timurid renaiss , later influenc age islam gunpowd empir .', \"asia 's histori featur major develop seen part world , well event affect region .\", 'includ trade silk road , spread cultur , languag , religion , diseas throughout afro-eurasian trade .', 'anoth major advanc innov gunpowd mediev china , later develop gunpowd empir , mainli mughal safavid , led advanc warfar use gun .']\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "paragraph = \"\"\"The history of Asia can be seen as the collective history of several distinct peripheral coastal regions such as East Asia, South Asia, Southeast Asia and the Middle East linked by the interior mass of the Eurasian steppe. See History of the Middle East and Outline of South Asian history for further details.\n",
        "\n",
        "The coastal periphery was the home to some of the world's earliest known civilizations and religions, with each of the three regions developing early civilizations around fertile river valleys. These valleys were fertile because the soil there was rich and could bear many root crops. The civilizations in Mesopotamia, India, and China shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Other notions such as that of writing likely developed individually in each area. Cities, states, and then empires developed in these lowlands.\n",
        "\n",
        "The steppe region had long been inhabited by mounted nomads, and from the central steppes, they could reach all areas of the Asian continent. The northern part of the continent, covering much of Siberia was also inaccessible to the steppe nomads due to the dense forests and the tundra. These areas in Siberia were very sparsely populated.\n",
        "\n",
        "The centre and periphery were kept separate by mountains and deserts. The Caucasus, Himalaya, Karakum Desert, and Gobi Desert formed barriers that the steppe horsemen could only cross with difficulty. While technologically and culturally the city dwellers were more advanced, they could do little militarily to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force. Thus the nomads who conquered states in the Middle East were soon forced to adapt to the local societies.\n",
        "\n",
        "The spread of Islam waved the Islamic Golden Age and the Timurid Renaissance, which later influenced the age of Islamic gunpowder empires.\n",
        "\n",
        "Asia's history features major developments seen in other parts of the world, as well as events that have affected those other regions. These include the trade of the Silk Road, which spread cultures, languages, religions, and diseases throughout Afro-Eurasian trade. Another major advancement was the innovation of gunpowder in medieval China, later developed by the Gunpowder empires, mainly by the Mughals and Safavids, which led to advanced warfare through the use of guns.\"\"\"\n",
        "               \n",
        "               \n",
        "newParagraph = nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming\n",
        "for i in range(len(newParagraph)):\n",
        "    words_tkn = nltk.word_tokenize(newParagraph[i])\n",
        "    words_tkn= [stemmer.stem(word) for word in words_tkn if word not in set(stopwords.words('english'))]\n",
        "    newParagraph[i] = ' '.join(words_tkn)   \n",
        "\n",
        "print(\"New paragraph :\",newParagraph)\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(newParagraph)):\n",
        "    words_tkn = nltk.word_tokenize(newParagraph[i])\n",
        "    words_tkn = [lemmatizer.lemmatize(word) for word in words_tkn if word not in set(stopwords.words('english'))]\n",
        "    newParagraph[i] = ' '.join(words_tkn)\n",
        "\n",
        "print(\"New paragraph:\",newParagraph)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#corpus \n",
        "corpus = []\n",
        "for i in range(len(newParagraph)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', newParagraph[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "corpus\n",
        "\n",
        "#TF-IDF model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPMes1GeU4BE",
        "outputId": "c4ef4894-0f4f-4832-90f6-a4f5f21849c7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.22165565 0.        ]\n",
            " ...\n",
            " [0.         0.         0.32548313 ... 0.         0.2855229  0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.39695544 0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "text_new = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
        "text_new = re.sub(r'\\s+',' ',text_new)\n",
        "text_new = text_new.lower()\n",
        "text_new = re.sub(r'\\d',' ',text_new)\n",
        "text_new = re.sub(r'\\s+',' ',text_new)\n",
        "\n",
        "text_new = re.sub(r'\\[[0-9]*\\]',' ',paragraph)\n",
        "text_new = re.sub(r'\\s+',' ',text_new)\n",
        "text_new = text_new.lower()\n",
        "text_new = re.sub(r'\\d',' ',text_new)\n",
        "text_new = re.sub(r'\\s+',' ',text_new)\n",
        "\n",
        "sentences = nltk.sent_tokenize(text_new)\n",
        "\n",
        "sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]\n",
        "\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "\n",
        "words = model.wv.vocab\n",
        "\n",
        "\n",
        "vector = model.wv['asia']\n",
        "vector\n",
        "\n",
        "similar = model.wv.most_similar('history')\n",
        "similar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YYOqhsyXiUl",
        "outputId": "6bb6e0d7-cb30-4b41-aec2-c9087a45939f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('home', 0.2747384309768677),\n",
              " ('nomads', 0.2474381923675537),\n",
              " ('empires', 0.23176008462905884),\n",
              " ('himalaya', 0.23027876019477844),\n",
              " ('mass', 0.20280273258686066),\n",
              " ('civilizations', 0.18868593871593475),\n",
              " ('languages', 0.18546587228775024),\n",
              " ('river', 0.18338727951049805),\n",
              " ('asia', 0.18324604630470276),\n",
              " ('asian', 0.176827073097229)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lkF5Jvqu08_"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text_new classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HHOJdQUEbWSz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In-class-exercise-03-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}